{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "from sklearn import metrics \n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tables import *\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.backend import manual_variable_initialization \n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, concatenate, Flatten, Conv1D, BatchNormalization, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, TerminateOnNaN\n",
    "#import keras\n",
    "\n",
    "#root\n",
    "absPath = '/home/angela3/imbalance_pcm_benchmark/'\n",
    "sys.path.insert(0, absPath)\n",
    "\n",
    "\n",
    "from src.model_functions import *\n",
    "from src.Target import Target\n",
    "from src.postproc_auxiliar_functions import *\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0' \n",
    "np.random.seed(8)\n",
    "random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/angela3/imbalance_pcm_benchmark/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "batch_size = 128\n",
    "epochss = 100\n",
    "type_padding_prot = \"pre_padding\"\n",
    "protein_type = \"GPCRs\" #\"kinases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading maximum lengths of proteins and compounds\n",
    "with open(\"\".join((absPath, 'data/prot_max_len.pickle')), \"rb\") as input_file:\n",
    "    max_len_prot = pickle.load(input_file)\n",
    "#Defining protein dictionary    \n",
    "instarget = Target(\"AAA\")\n",
    "prot_dict = instarget.predefining_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "decay_rate = learning_rate/epochss\n",
    "adamm = Adam(lr=learning_rate, beta_1=0.1, beta_2=0.001, epsilon=1e-08, decay=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1499, 26)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1499, 64)     5056        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1499, 64)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 95936)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 881)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           4796850     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           44100       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100)          0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            202         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,846,208\n",
      "Trainable params: 4,846,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LEFT BLOCK (to analyse amino acid sequences)\n",
    "input_seq = Input(shape=(max_len_prot, len(prot_dict)), dtype='float32')\n",
    "conv_seq = Conv1D(filters=64, padding='same', strides=1, kernel_size=3, activation='relu')(input_seq)\n",
    "dropout_1 = Dropout(0.4)(conv_seq)\n",
    "flatten_seq = Flatten()(dropout_1)#(dense_seq)\n",
    "dense_seq_2 = Dense(50)(flatten_seq)\n",
    "dropout_2 = Dropout(0.4)(dense_seq_2)\n",
    "\n",
    "#RIGHT BRANCH (to analyse fingerprints)\n",
    "input_fps = Input(shape=(881,), dtype='float32')\n",
    "dense_fps = Dense(50)(input_fps)\n",
    "dropout_3 = Dropout(0.4)(dense_fps)\n",
    "#bn_3 =  BatchNormalization()(dense_fps)#(dense_seq_2)#(conv_seq)\n",
    "\n",
    "\n",
    "#MERGE BOTH BRANCHES\n",
    "main_merged = concatenate([dropout_2, dropout_3],axis=1)#([dense_seq_2, dense_fps], axis=1)\n",
    "\n",
    "main_dense = Dense(2, activation='softmax')(main_merged)\n",
    "\n",
    "#build and compile model\n",
    "model = Model(inputs=[input_seq, input_fps], outputs=[main_dense])\n",
    "model.compile(loss='categorical_crossentropy', optimizer = adamm, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_h5 = open_file(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/\", str(fold),\n",
    "#                                 \"/compounds_activity.h5\")), \"w\")\n",
    "\n",
    "#pickle_filename = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/\", str(fold), \n",
    "#                               \"/splitting_lists/\", column_name, \"_list.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Training\n",
      "Epoch 1/100\n",
      "1829/1829 [==============================] - 157s 86ms/step - loss: 1.6232 - accuracy: 0.6334 - val_loss: 0.4935 - val_accuracy: 0.6245\n",
      "Epoch 2/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6487 - accuracy: 0.6545 - val_loss: 0.5548 - val_accuracy: 0.6165E\n",
      "Epoch 3/100\n",
      "1829/1829 [==============================] - 151s 82ms/step - loss: 0.6497 - accuracy: 0.6580 - val_loss: 0.6475 - val_accuracy: 0.5986\n",
      "Epoch 4/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6487 - accuracy: 0.6600 - val_loss: 0.7728 - val_accuracy: 0.5929\n",
      "Epoch 5/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6452 - accuracy: 0.6624 - val_loss: 0.5269 - val_accuracy: 0.5886\n",
      "Epoch 6/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6436 - accuracy: 0.6627 - val_loss: 0.6971 - val_accuracy: 0.5499\n",
      "Epoch 7/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6464 - accuracy: 0.6627 - val_loss: 0.6508 - val_accuracy: 0.5874\n",
      "Epoch 8/100\n",
      "1829/1829 [==============================] - 153s 84ms/step - loss: 0.6494 - accuracy: 0.6630 - val_loss: 1.0136 - val_accuracy: 0.6172\n",
      "Epoch 9/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6501 - accuracy: 0.6627 - val_loss: 1.1095 - val_accuracy: 0.6332\n",
      "Epoch 10/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6518 - accuracy: 0.6630 - val_loss: 0.4845 - val_accuracy: 0.6354\n",
      "Epoch 11/100\n",
      "1829/1829 [==============================] - 151s 83ms/step - loss: 0.6541 - accuracy: 0.6623 - val_loss: 0.3605 - val_accuracy: 0.6125\n",
      "Epoch 12/100\n",
      "1829/1829 [==============================] - 151s 82ms/step - loss: 0.6516 - accuracy: 0.6626 - val_loss: 0.6191 - val_accuracy: 0.5752\n",
      "Epoch 13/100\n",
      "1829/1829 [==============================] - 151s 83ms/step - loss: 0.6568 - accuracy: 0.6638 - val_loss: 0.7273 - val_accuracy: 0.5771\n",
      "Epoch 14/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6543 - accuracy: 0.6646 - val_loss: 0.7342 - val_accuracy: 0.6154\n",
      "Epoch 15/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6596 - accuracy: 0.6643 - val_loss: 1.3644 - val_accuracy: 0.6374\n",
      "Epoch 16/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6575 - accuracy: 0.6649 - val_loss: 0.4381 - val_accuracy: 0.6214\n",
      "Epoch 17/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6562 - accuracy: 0.6661 - val_loss: 0.5942 - val_accuracy: 0.6152\n",
      "Epoch 18/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6607 - accuracy: 0.6641 - val_loss: 0.7188 - val_accuracy: 0.5729\n",
      "Epoch 19/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6580 - accuracy: 0.6648 - val_loss: 0.5639 - val_accuracy: 0.5853\n",
      "Epoch 20/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6609 - accuracy: 0.6665 - val_loss: 0.5154 - val_accuracy: 0.5864\n",
      "Epoch 21/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6615 - accuracy: 0.6642 - val_loss: 0.5528 - val_accuracy: 0.5018\n",
      "Epoch 22/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6622 - accuracy: 0.6659 - val_loss: 0.7139 - val_accuracy: 0.5665\n",
      "Epoch 23/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6658 - accuracy: 0.6630 - val_loss: 0.8844 - val_accuracy: 0.6154\n",
      "Epoch 24/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6585 - accuracy: 0.6643 - val_loss: 0.5579 - val_accuracy: 0.64360.6587 - accu - ETA: 5s - ETA: 3s - loss: 0.658 - ETA: 3s\n",
      "Epoch 25/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6611 - accuracy: 0.6648 - val_loss: 0.3937 - val_accuracy: 0.6480\n",
      "Epoch 26/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6578 - accuracy: 0.6665 - val_loss: 0.3498 - val_accuracy: 0.6300\n",
      "Epoch 27/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6607 - accuracy: 0.6646 - val_loss: 0.6991 - val_accuracy: 0.6302\n",
      "Epoch 28/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6602 - accuracy: 0.6665 - val_loss: 0.6241 - val_accuracy: 0.6144\n",
      "Epoch 29/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6620 - accuracy: 0.6641 - val_loss: 0.6710 - val_accuracy: 0.6450\n",
      "Epoch 30/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6607 - accuracy: 0.6664 - val_loss: 0.5187 - val_accuracy: 0.6490\n",
      "Epoch 31/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6649 - accuracy: 0.6640 - val_loss: 0.5567 - val_accuracy: 0.6462\n",
      "Epoch 32/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6612 - accuracy: 0.6666 - val_loss: 1.2569 - val_accuracy: 0.6249\n",
      "Epoch 33/100\n",
      "1829/1829 [==============================] - 153s 84ms/step - loss: 0.6607 - accuracy: 0.6672 - val_loss: 0.4135 - val_accuracy: 0.6345\n",
      "Epoch 34/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6638 - accuracy: 0.6660 - val_loss: 0.5264 - val_accuracy: 0.6236\n",
      "Epoch 35/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6653 - accuracy: 0.6657 - val_loss: 0.7584 - val_accuracy: 0.6513\n",
      "Epoch 36/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6619 - accuracy: 0.6669 - val_loss: 0.7577 - val_accuracy: 0.6599\n",
      "Epoch 37/100\n",
      "1829/1829 [==============================] - 153s 84ms/step - loss: 0.6661 - accuracy: 0.6646 - val_loss: 0.7373 - val_accuracy: 0.6602\n",
      "Epoch 38/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6629 - accuracy: 0.6677 - val_loss: 0.4072 - val_accuracy: 0.6567\n",
      "Epoch 39/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6619 - accuracy: 0.6666 - val_loss: 1.1655 - val_accuracy: 0.6592 loss: 0.6622 - accuracy: \n",
      "Epoch 40/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6611 - accuracy: 0.6681 - val_loss: 0.4649 - val_accuracy: 0.6478\n",
      "Epoch 41/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6621 - accuracy: 0.6657 - val_loss: 0.7782 - val_accuracy: 0.6233\n",
      "Epoch 42/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6614 - accuracy: 0.6692 - val_loss: 1.1111 - val_accuracy: 0.5889\n",
      "Epoch 43/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6615 - accuracy: 0.6670 - val_loss: 0.7416 - val_accuracy: 0.4761\n",
      "Epoch 44/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6624 - accuracy: 0.6670 - val_loss: 0.6747 - val_accuracy: 0.4680\n",
      "Epoch 45/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6607 - accuracy: 0.6674 - val_loss: 0.8711 - val_accuracy: 0.4175\n",
      "Epoch 46/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6624 - accuracy: 0.6665 - val_loss: 0.5508 - val_accuracy: 0.5842\n",
      "Epoch 47/100\n",
      "1829/1829 [==============================] - 155s 84ms/step - loss: 0.6628 - accuracy: 0.6654 - val_loss: 0.7282 - val_accuracy: 0.6362\n",
      "Epoch 48/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6635 - accuracy: 0.6654 - val_loss: 0.7275 - val_accuracy: 0.6394\n",
      "Epoch 49/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6618 - accuracy: 0.6660 - val_loss: 0.7992 - val_accuracy: 0.6518\n",
      "Epoch 50/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6656 - accuracy: 0.6668 - val_loss: 0.7872 - val_accuracy: 0.6585\n",
      "Epoch 51/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6626 - accuracy: 0.6667 - val_loss: 0.5495 - val_accuracy: 0.6537\n",
      "Epoch 52/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6617 - accuracy: 0.6674 - val_loss: 0.6055 - val_accuracy: 0.6541\n",
      "Epoch 53/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6640 - accuracy: 0.6659 - val_loss: 0.3523 - val_accuracy: 0.6562\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6602 - accuracy: 0.6673 - val_loss: 0.3942 - val_accuracy: 0.6553\n",
      "Epoch 55/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6625 - accuracy: 0.6667 - val_loss: 1.2059 - val_accuracy: 0.6581\n",
      "Epoch 56/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6598 - accuracy: 0.6668 - val_loss: 0.1371 - val_accuracy: 0.6599\n",
      "Epoch 57/100\n",
      "1829/1829 [==============================] - 153s 84ms/step - loss: 0.6635 - accuracy: 0.6668 - val_loss: 0.8142 - val_accuracy: 0.6564\n",
      "Epoch 58/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6637 - accuracy: 0.6663 - val_loss: 0.9798 - val_accuracy: 0.6547\n",
      "Epoch 59/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6583 - accuracy: 0.6677 - val_loss: 0.6687 - val_accuracy: 0.6551\n",
      "Epoch 60/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6612 - accuracy: 0.6666 - val_loss: 0.6998 - val_accuracy: 0.6528\n",
      "Epoch 61/100\n",
      "1829/1829 [==============================] - 157s 86ms/step - loss: 0.6638 - accuracy: 0.6655 - val_loss: 0.7467 - val_accuracy: 0.6121\n",
      "Epoch 62/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6643 - accuracy: 0.6657 - val_loss: 0.5712 - val_accuracy: 0.6553\n",
      "Epoch 63/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6630 - accuracy: 0.6661 - val_loss: 0.8255 - val_accuracy: 0.6616\n",
      "Epoch 64/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6609 - accuracy: 0.6661 - val_loss: 0.5113 - val_accuracy: 0.6604\n",
      "Epoch 65/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6626 - accuracy: 0.6662 - val_loss: 0.7084 - val_accuracy: 0.6450\n",
      "Epoch 66/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6604 - accuracy: 0.6690 - val_loss: 0.8297 - val_accuracy: 0.6273\n",
      "Epoch 67/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6616 - accuracy: 0.6683 - val_loss: 0.4485 - val_accuracy: 0.6583\n",
      "Epoch 68/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6630 - accuracy: 0.6662 - val_loss: 0.7609 - val_accuracy: 0.6528\n",
      "Epoch 69/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6643 - accuracy: 0.6653 - val_loss: 0.4904 - val_accuracy: 0.6570\n",
      "Epoch 70/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6615 - accuracy: 0.6664 - val_loss: 0.6682 - val_accuracy: 0.6461\n",
      "Epoch 71/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6617 - accuracy: 0.6675 - val_loss: 0.6392 - val_accuracy: 0.6102\n",
      "Epoch 72/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6613 - accuracy: 0.6677 - val_loss: 1.1006 - val_accuracy: 0.6272\n",
      "Epoch 73/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6576 - accuracy: 0.6703 - val_loss: 1.7020 - val_accuracy: 0.6553\n",
      "Epoch 74/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6621 - accuracy: 0.6675 - val_loss: 1.2247 - val_accuracy: 0.6503\n",
      "Epoch 75/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6641 - accuracy: 0.6664 - val_loss: 0.9588 - val_accuracy: 0.6579\n",
      "Epoch 76/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6626 - accuracy: 0.6667 - val_loss: 0.6839 - val_accuracy: 0.6624\n",
      "Epoch 77/100\n",
      "1829/1829 [==============================] - 155s 84ms/step - loss: 0.6612 - accuracy: 0.6676 - val_loss: 0.7204 - val_accuracy: 0.6598\n",
      "Epoch 78/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6607 - accuracy: 0.6672 - val_loss: 0.5801 - val_accuracy: 0.6524\n",
      "Epoch 79/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6592 - accuracy: 0.6701 - val_loss: 0.5129 - val_accuracy: 0.6400\n",
      "Epoch 80/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6619 - accuracy: 0.6681 - val_loss: 0.7028 - val_accuracy: 0.5949\n",
      "Epoch 81/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6629 - accuracy: 0.6673 - val_loss: 0.5573 - val_accuracy: 0.6225\n",
      "Epoch 82/100\n",
      "1829/1829 [==============================] - 154s 84ms/step - loss: 0.6598 - accuracy: 0.6692 - val_loss: 0.7817 - val_accuracy: 0.6582\n",
      "Epoch 83/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6635 - accuracy: 0.6676 - val_loss: 0.5496 - val_accuracy: 0.6128\n",
      "Epoch 84/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6621 - accuracy: 0.6658 - val_loss: 0.6107 - val_accuracy: 0.5896\n",
      "Epoch 85/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6585 - accuracy: 0.6666 - val_loss: 0.7091 - val_accuracy: 0.5983\n",
      "Epoch 86/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6595 - accuracy: 0.6681 - val_loss: 0.6101 - val_accuracy: 0.6368\n",
      "Epoch 87/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6564 - accuracy: 0.6687 - val_loss: 0.7017 - val_accuracy: 0.5984\n",
      "Epoch 88/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6556 - accuracy: 0.6685 - val_loss: 0.9654 - val_accuracy: 0.6250\n",
      "Epoch 89/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6583 - accuracy: 0.6678 - val_loss: 0.6369 - val_accuracy: 0.6584\n",
      "Epoch 90/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6533 - accuracy: 0.6689 - val_loss: 0.6377 - val_accuracy: 0.6622\n",
      "Epoch 91/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6602 - accuracy: 0.6667 - val_loss: 0.2605 - val_accuracy: 0.6581\n",
      "Epoch 92/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6544 - accuracy: 0.6697 - val_loss: 0.7799 - val_accuracy: 0.6589\n",
      "Epoch 93/100\n",
      "1829/1829 [==============================] - 155s 84ms/step - loss: 0.6550 - accuracy: 0.6690 - val_loss: 0.8507 - val_accuracy: 0.6608\n",
      "Epoch 94/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6581 - accuracy: 0.6675 - val_loss: 0.3130 - val_accuracy: 0.6544\n",
      "Epoch 95/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6609 - accuracy: 0.6665 - val_loss: 0.6681 - val_accuracy: 0.6632\n",
      "Epoch 96/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6561 - accuracy: 0.6707 - val_loss: 0.9651 - val_accuracy: 0.6589\n",
      "Epoch 97/100\n",
      "1829/1829 [==============================] - 157s 86ms/step - loss: 0.6545 - accuracy: 0.6689 - val_loss: 0.6995 - val_accuracy: 0.6608\n",
      "Epoch 98/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6592 - accuracy: 0.6687 - val_loss: 0.4548 - val_accuracy: 0.6600\n",
      "Epoch 99/100\n",
      "1829/1829 [==============================] - 155s 85ms/step - loss: 0.6598 - accuracy: 0.6689 - val_loss: 0.5054 - val_accuracy: 0.6538\n",
      "Epoch 100/100\n",
      "1829/1829 [==============================] - 156s 85ms/step - loss: 0.6568 - accuracy: 0.6686 - val_loss: 0.6785 - val_accuracy: 0.6618s - loss: 0.656\n",
      "Prediction on test data\n",
      "best epoch:  095\n",
      "Counting predicted:  Counter({1: 13667, 0: 7468})\n",
      "\n",
      "Model Report\n",
      "Accuracy (test set): 0.6052\n",
      "Confusion matrix:\n",
      "[[ 1994  2871]\n",
      " [ 5474 10796]]\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.41      0.32      4865\n",
      "           1       0.79      0.66      0.72     16270\n",
      "\n",
      "    accuracy                           0.61     21135\n",
      "   macro avg       0.53      0.54      0.52     21135\n",
      "weighted avg       0.67      0.61      0.63     21135\n",
      "\n",
      "\n",
      "Model Report II part\n",
      "AUC Score (Test): 0.565357\n",
      "Fold: 1\n",
      "Training\n",
      "Epoch 1/100\n",
      "1873/1873 [==============================] - 155s 83ms/step - loss: 0.6837 - accuracy: 0.6467 - val_loss: 0.6428 - val_accuracy: 0.6925\n",
      "Epoch 2/100\n",
      "1873/1873 [==============================] - 146s 78ms/step - loss: 0.6732 - accuracy: 0.6499 - val_loss: 0.7306 - val_accuracy: 0.6936\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6740 - accuracy: 0.6498 - val_loss: 0.6819 - val_accuracy: 0.6857\n",
      "Epoch 4/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6696 - accuracy: 0.6553 - val_loss: 0.6203 - val_accuracy: 0.6872\n",
      "Epoch 5/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6732 - accuracy: 0.6525 - val_loss: 0.6268 - val_accuracy: 0.6706\n",
      "Epoch 6/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6644 - accuracy: 0.6575 - val_loss: 0.5819 - val_accuracy: 0.6813s: 0.6573 - acc\n",
      "Epoch 7/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6634 - accuracy: 0.6588 - val_loss: 0.6749 - val_accuracy: 0.6839\n",
      "Epoch 8/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6682 - accuracy: 0.6561 - val_loss: 0.5606 - val_accuracy: 0.6842 1:0\n",
      "Epoch 9/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6648 - accuracy: 0.6578 - val_loss: 0.6412 - val_accuracy: 0.68171s - loss: 0. - ETA: 19s - loss: 0.6682 - accuracy - ETA: 19s - loss: - ETA: 0s - loss: 0.6648 - accuracy: 0.\n",
      "Epoch 10/100\n",
      "1873/1873 [==============================] - 147s 78ms/step - loss: 0.6633 - accuracy: 0.6617 - val_loss: 0.4147 - val_accuracy: 0.6511\n",
      "Epoch 11/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6620 - accuracy: 0.6594 - val_loss: 0.4908 - val_accuracy: 0.5991\n",
      "Epoch 12/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6591 - accuracy: 0.6639 - val_loss: 0.6556 - val_accuracy: 0.5344\n",
      "Epoch 13/100\n",
      "1873/1873 [==============================] - 147s 78ms/step - loss: 0.6593 - accuracy: 0.6635 - val_loss: 0.3288 - val_accuracy: 0.6441\n",
      "Epoch 14/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6592 - accuracy: 0.6634 - val_loss: 1.4122 - val_accuracy: 0.6814\n",
      "Epoch 15/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6643 - accuracy: 0.6613 - val_loss: 0.4129 - val_accuracy: 0.6841\n",
      "Epoch 16/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6615 - accuracy: 0.6628 - val_loss: 0.0422 - val_accuracy: 0.6863\n",
      "Epoch 17/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6573 - accuracy: 0.6650 - val_loss: 0.2776 - val_accuracy: 0.6881\n",
      "Epoch 18/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6585 - accuracy: 0.6645 - val_loss: 0.5370 - val_accuracy: 0.6829\n",
      "Epoch 19/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6602 - accuracy: 0.6641 - val_loss: 0.8295 - val_accuracy: 0.6355 - ETA: 58s - - ETA: 56s - loss: 0.6509 - ETA: 55s - loss: 0.65 - ETA:  - ETA: 51s - loss: 0.6475 - a - ETA: 50s - - ETA: 47s - loss: 0. - ETA: 42s - loss\n",
      "Epoch 20/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6595 - accuracy: 0.6647 - val_loss: 0.6446 - val_accuracy: 0.5914\n",
      "Epoch 21/100\n",
      "1873/1873 [==============================] - 147s 78ms/step - loss: 0.6610 - accuracy: 0.6641 - val_loss: 0.7169 - val_accuracy: 0.6073\n",
      "Epoch 22/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6602 - accuracy: 0.6645 - val_loss: 0.6602 - val_accuracy: 0.6648E\n",
      "Epoch 23/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6605 - accuracy: 0.6644 - val_loss: 0.6724 - val_accuracy: 0.6657\n",
      "Epoch 24/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6586 - accuracy: 0.6640 - val_loss: 0.5866 - val_accuracy: 0.6287TA: 21s - loss: 0.6607 - accura - ETA: 20s - loss: 0.6602 - a - ETA: 19s - loss: 0.6610 - accuracy: 0.6 - ETA: 19s - loss: 0.66 - ETA: 17s -  - ETA\n",
      "Epoch 25/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6582 - accuracy: 0.6655 - val_loss: 1.2268 - val_accuracy: 0.6531\n",
      "Epoch 26/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6552 - accuracy: 0.6679 - val_loss: 0.7963 - val_accuracy: 0.6300.6 - ETA: 51s - loss: 0.6451 - accuracy - ETA: 50s -  - ETA: 24s - loss: 0.6582 - accuracy: 0. - ETA: 24s - loss: 0.657 - ETA: 22s - loss: 0.6573 - accu - ETA: 21s - loss\n",
      "Epoch 27/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6577 - accuracy: 0.6658 - val_loss: 0.6966 - val_accuracy: 0.6557\n",
      "Epoch 28/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6571 - accuracy: 0.6668 - val_loss: 0.6783 - val_accuracy: 0.6586\n",
      "Epoch 29/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6555 - accuracy: 0.6673 - val_loss: 0.6964 - val_accuracy: 0.6558loss: 0.6\n",
      "Epoch 30/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6594 - accuracy: 0.6660 - val_loss: 0.5436 - val_accuracy: 0.5991: 50s - loss: 0.6513 - ac - ETA: 49s  - ET - ETA: 33s - loss:  - ETA: 31s - loss: 0.6632  - ETA: 30s - loss: 0.6634 - accuracy: 0 - ETA: 30s - loss: 0.6635 - accuracy - ETA: 29s - ETA: 26s - loss:  - ETA: 24s - loss: 0.6629 - accuracy: 0.662 - ETA: - ETA: 18s - loss: 0.6644 - accuracy:  - ETA: 18s - loss: 0.6643 - accurac - ETA: 17s - loss: 0.6642 -  - ETA: 13s - loss: 0.6637 - accuracy: - ETA: 12s - loss: 0.6634 - accuracy: 0.6 - ETA: 12s - loss: 0.6630 - accuracy: 0. - E\n",
      "Epoch 31/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6589 - accuracy: 0.6654 - val_loss: 0.4222 - val_accuracy: 0.4568loss: 0.6621 - accuracy:\n",
      "Epoch 32/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6590 - accuracy: 0.6651 - val_loss: 0.5160 - val_accuracy: 0.5999uracy: 0.66 - ETA: 25s - loss: 0.6623 - a - ETA: 24s - loss: 0.6615 -  - ETA: 23s - loss: 0.6622 - accuracy: - ETA: 22s - l - ETA\n",
      "Epoch 33/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6582 - accuracy: 0.6653 - val_loss: 1.6848 - val_accuracy: 0.6518\n",
      "Epoch 34/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6574 - accuracy: 0.6679 - val_loss: 0.5674 - val_accuracy: 0.6145\n",
      "Epoch 35/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6563 - accuracy: 0.6662 - val_loss: 0.5485 - val_accuracy: 0.6603\n",
      "Epoch 36/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6566 - accuracy: 0.6666 - val_loss: 0.7176 - val_accuracy: 0.6718\n",
      "Epoch 37/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6582 - accuracy: 0.6662 - val_loss: 0.4680 - val_accuracy: 0.67940.6580  - ETA: 0s - loss: 0.6585 - accuracy:  - ETA: 0s - loss: 0.6586 - ac - ETA: 0s - loss: 0.6582 - accuracy: \n",
      "Epoch 38/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6552 - accuracy: 0.6676 - val_loss: 1.3489 - val_accuracy: 0.6886\n",
      "Epoch 39/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6575 - accuracy: 0.6668 - val_loss: 0.7552 - val_accuracy: 0.6963\n",
      "Epoch 40/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6571 - accuracy: 0.6674 - val_loss: 0.9410 - val_accuracy: 0.6856 - ETA: 2s - loss: 0.6567 - accu - ETA: 0s - loss: 0.6574 - accu\n",
      "Epoch 41/100\n",
      "1873/1873 [==============================] - 151s 80ms/step - loss: 0.6546 - accuracy: 0.6687 - val_loss: 0.9138 - val_accuracy: 0.6773\n",
      "Epoch 42/100\n",
      "1873/1873 [==============================] - 147s 78ms/step - loss: 0.6536 - accuracy: 0.6681 - val_loss: 0.4002 - val_accuracy: 0.6751\n",
      "Epoch 43/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6566 - accuracy: 0.6667 - val_loss: 0.4734 - val_accuracy: 0.6854\n",
      "Epoch 44/100\n",
      "1873/1873 [==============================] - 151s 80ms/step - loss: 0.6593 - accuracy: 0.6654 - val_loss: 0.4307 - val_accuracy: 0.6831\n",
      "Epoch 45/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6574 - accuracy: 0.6673 - val_loss: 0.5147 - val_accuracy: 0.6764\n",
      "Epoch 46/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6551 - accuracy: 0.6676 - val_loss: 0.4516 - val_accuracy: 0.6293\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6554 - accuracy: 0.6664 - val_loss: 0.5582 - val_accuracy: 0.6258A: 1:12 - loss: 0.6374 - ac - ETA:  - ETA: 1:10 - loss: 0.6372 - accuracy:  - ETA: 1:10 - loss: 0.6374 - ac - ETA: 1:09 - loss: 0.63 - ETA: 46s - loss: 0.6475 - accuracy: 0.66 - ETA: 46s - loss: 0. - ETA: 44s - loss: 0.6481 - acc - ETA: 40s - l - ETA: 38s - l - ETA: 29 - ETA:  - ETA: 0s - loss: 0.654\n",
      "Epoch 48/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6536 - accuracy: 0.6676 - val_loss: 0.6242 - val_accuracy: 0.6535\n",
      "Epoch 49/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6587 - accuracy: 0.6652 - val_loss: 0.3824 - val_accuracy: 0.6758\n",
      "Epoch 50/100\n",
      "1873/1873 [==============================] - 151s 80ms/step - loss: 0.6598 - accuracy: 0.6670 - val_loss: 0.6809 - val_accuracy: 0.6691\n",
      "Epoch 51/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6598 - accuracy: 0.6647 - val_loss: 0.6297 - val_accuracy: 0.6416ss: 0.6357 -  - ETA: 1:26 - los - ETA: 1:25 - loss: 0 - ETA: 1:17 - loss: 0.6474 - accuracy: 0.66 - ETA: 1:17 - loss: 0.6 - ETA: 1:16 - - ETA - ETA:  - ETA: 1:02 - loss: 0.6543 - accuracy: 0. - ETA: 1:02 - loss: - ETA:  - ETA: 6s -\n",
      "Epoch 52/100\n",
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6564 - accuracy: 0.6643 - val_loss: 0.9621 - val_accuracy: 0.6462\n",
      "Epoch 53/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6571 - accuracy: 0.6657 - val_loss: 0.8703 - val_accuracy: 0.6483ETA: 46s - loss: 0.6502 - accuracy - ETA: 46s - lo  - ETA: 34s -  - ETA: 1s - l\n",
      "Epoch 54/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6561 - accuracy: 0.6664 - val_loss: 0.5304 - val_accuracy: 0.6403\n",
      "Epoch 55/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6543 - accuracy: 0.6662 - val_loss: 0.6609 - val_accuracy: 0.541030s - ETA: 21s - loss: 0.6594 - accuracy - ETA: 20s - loss: 0.6591 - accuracy: 0. - ETA: 2 - ET - ETA: 2s - loss: 0.654\n",
      "Epoch 56/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6571 - accuracy: 0.6652 - val_loss: 0.6136 - val_accuracy: 0.6203\n",
      "Epoch 57/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6542 - accuracy: 0.6676 - val_loss: 0.6507 - val_accuracy: 0.6601\n",
      "Epoch 58/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6556 - accuracy: 0.6668 - val_loss: 0.4416 - val_accuracy: 0.6742\n",
      "Epoch 59/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6550 - accuracy: 0.6675 - val_loss: 0.4874 - val_accuracy: 0.6452502 - accuracy:  - ETA: 50s - los - ETA: 28s - ETA: 25s - loss: 0.6627 - accuracy - ETA: 25s - loss: 0.662 - ETA: 23s - loss: 0.6625 - a - ETA: 22s - loss: 0.6605 - accuracy: 0. -  - ETA\n",
      "Epoch 60/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6562 - accuracy: 0.6663 - val_loss: 0.5051 - val_accuracy: 0.6066\n",
      "Epoch 61/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6552 - accuracy: 0.6663 - val_loss: 0.4682 - val_accuracy: 0.6360\n",
      "Epoch 62/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6542 - accuracy: 0.6672 - val_loss: 0.5090 - val_accuracy: 0.6264 accuracy\n",
      "Epoch 63/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6553 - accuracy: 0.6663 - val_loss: 0.4925 - val_accuracy: 0.6123\n",
      "Epoch 64/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6527 - accuracy: 0.6682 - val_loss: 0.5825 - val_accuracy: 0.6646\n",
      "Epoch 65/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6517 - accuracy: 0.6673 - val_loss: 1.1002 - val_accuracy: 0.6780ss: 0.6437 -  - ETA:  - ETA: 41s - loss: 0.6455 - accuracy: 0.66 - ETA: 41s - loss - ETA: 39 - ETA: 36s - loss: 0 - ETA: 34s - loss: 0.6542 - accuracy: - ETA: 34s - loss: 0.6542 -  - ETA: 33s - loss: 0.6550 - accuracy: 0. - ETA: 32s - loss: 0.6545 - a - ETA: 31s - loss: 0.6 - ETA: 30s - - ETA: 24s - loss: 0.6581 - accuracy: 0. - ETA: 24s - loss: 0.6578  - ETA: 23s - loss: 0.6570 - accu - ETA: 22s - loss: 0.6557 - accuracy: 0.6 - ETA: 22s - loss:  - ETA - ETA - ETA - ETA: 5s - loss: 0.6554 - accu - ETA: 5s - - ETA:  - ETA: 2s - loss: 0 - ETA: 2s - loss: 0 - ETA: 1s - loss: 0.6524 - accura - ETA: 0s - loss: 0.6522 - accuracy: 0.66 - ETA: 0s - loss: 0.6522 \n",
      "Epoch 66/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6535 - accuracy: 0.6669 - val_loss: 0.6448 - val_accuracy: 0.6564\n",
      "Epoch 67/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6531 - accuracy: 0.6657 - val_loss: 0.6569 - val_accuracy: 0.6417\n",
      "Epoch 68/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6554 - accuracy: 0.6650 - val_loss: 0.4488 - val_accuracy: 0.6690 ETA: 6s - loss: 0.6589 - accuracy: 0.66 - ETA: 6s - loss: 0 - ETA: 5s - loss: 0.6590 - accura - ETA: 5s - loss: 0.659 - E\n",
      "Epoch 69/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6527 - accuracy: 0.6683 - val_loss: 0.7231 - val_accuracy: 0.6825\n",
      "Epoch 70/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6531 - accuracy: 0.6662 - val_loss: 0.5245 - val_accuracy: 0.6821\n",
      "Epoch 71/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6544 - accuracy: 0.6662 - val_loss: 0.5039 - val_accuracy: 0.68565s - loss: 0.6480 -  - ET - ETA: 48s - loss: 0.6459 - accuracy:  - ETA: 47s - loss:  - ETA: 45s - loss: 0.6469 - accuracy: 0 - ETA:  - ETA: 42s - los - ETA: 40s - loss: 0.6478 - acc\n",
      "Epoch 72/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6577 - accuracy: 0.6679 - val_loss: 0.9334 - val_accuracy: 0.671541s - loss: 0.6483 - accuracy: 0.670 - ETA: 41s - loss: 0.6483 - - ETA: 30s - loss: 0.660 - ETA: 28s - loss: 0.6613 - accuracy: 0 - ETA: 28s - loss:  - ETA: 26s - loss: 0.6635 - accuracy: 0.662 - ETA: 26s \n",
      "Epoch 73/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6557 - accuracy: 0.6676 - val_loss: 0.7004 - val_accuracy: 0.6545\n",
      "Epoch 74/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6544 - accuracy: 0.6679 - val_loss: 0.9350 - val_accuracy: 0.6100\n",
      "Epoch 75/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6547 - accuracy: 0.6673 - val_loss: 0.9021 - val_accuracy: 0.6513\n",
      "Epoch 76/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6541 - accuracy: 0.6678 - val_loss: 1.1226 - val_accuracy: 0.6741\n",
      "Epoch 77/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6547 - accuracy: 0.6680 - val_loss: 1.2039 - val_accuracy: 0.6806\n",
      "Epoch 78/100\n",
      "1873/1873 [==============================] - 147s 78ms/step - loss: 0.6522 - accuracy: 0.6690 - val_loss: 1.5666 - val_accuracy: 0.6721- loss: 0.6570 - accuracy: 0.666 - ETA: 12s - loss - E - ETA: 7s - loss:\n",
      "Epoch 79/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6546 - accuracy: 0.6663 - val_loss: 1.0191 - val_accuracy: 0.6585.6440 - accuracy:  - ETA: 58s - loss: 0.6440 -  - ETA: 57s - loss: 0.6462 - accura - ETA: 56s - loss: 0 - ETA: 55s - loss: 0.6460 - accuracy: 0.6 - ETA: 54s - loss: 0.6460 - accura\n",
      "Epoch 80/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6528 - accuracy: 0.6688 - val_loss: 0.6414 - val_accuracy: 0.6256:18 - ETA: 1: - ETA: 1:16 - loss: 0.6275 - accuracy: 0. - ETA: 1:15 - loss: 0.6276 - accuracy: 0. - ETA: 21s - loss: 0.6588 - a - ETA: 20s - loss: 0.6581 -  - ETA - ETA: 8s - - ETA: 3s - loss: 0.655\n",
      "Epoch 81/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6546 - accuracy: 0.6680 - val_loss: 1.0310 - val_accuracy: 0.6366loss:  - ETA: 48s - loss: 0.6438 - a - ETA: 46s - ETA: 44s - lo - ETA: 42s - loss: 0. - ETA: 40s - loss: 0.6471  -  - ETA: 1s - loss: 0.6554  - ETA: 0s - loss: 0.6\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873/1873 [==============================] - 147s 79ms/step - loss: 0.6510 - accuracy: 0.6682 - val_loss: 0.6279 - val_accuracy: 0.6303\n",
      "Epoch 83/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6556 - accuracy: 0.6660 - val_loss: 0.7711 - val_accuracy: 0.66200.66 - ETA: 50s - loss: 0.6448 - accuracy: 0.67 - ETA: 50s - los - ETA: 48s - loss: 0.6 - ETA: 1s\n",
      "Epoch 84/100\n",
      "1873/1873 [==============================] - 150s 80ms/step - loss: 0.6553 - accuracy: 0.6684 - val_loss: 0.4523 - val_accuracy: 0.6680- loss: 0.6505 - accurac - ETA: 34 - ETA: 31 - ETA: 28s - loss: 0.6592 - accuracy: 0.666 - - ETA - ETA: 11s - loss: 0.6591 - accur - ETA: - ETA: 7s - loss: 0.6583 - accura - ETA: 5s - loss: 0.6 - ETA: 4s - loss: 0.6 - ETA: 3s - loss: 0.658 - ETA\n",
      "Epoch 85/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6510 - accuracy: 0.6691 - val_loss: 0.4109 - val_accuracy: 0.6760\n",
      "Epoch 86/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6541 - accuracy: 0.6675 - val_loss: 0.3123 - val_accuracy: 0.6807\n",
      "Epoch 87/100\n",
      "1873/1873 [==============================] - 150s 80ms/step - loss: 0.6520 - accuracy: 0.6686 - val_loss: 0.3796 - val_accuracy: 0.6636- ETA - ETA: 35s - loss: 0.6493 - accuracy: 0.6 - ETA: 34s - loss: 0.6490 - accuracy: 0.668 - ETA: 34s - loss: 0. - ETA: 33s - loss: - ETA - ETA: 24s - loss: 0.6574 - accuracy - - ETA: 21s - loss:  - ETA: - ETA: 16s - loss: 0.65 - ETA: 15s - loss: 0.6586 - accuracy: - ETA: 14s - loss: 0.6587 - ETA: 8s - l - ETA: 3s - l - ETA: 2s - loss: 0.6548 - ac - ETA: 2s - los - ETA: 1s - loss: 0.653 - ETA: 0s - loss: 0.6525 - accura\n",
      "Epoch 88/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6545 - accuracy: 0.6667 - val_loss: 0.5679 - val_accuracy: 0.6530accuracy: 0 - ETA: 29s - loss: 0.6\n",
      "Epoch 89/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6534 - accuracy: 0.6677 - val_loss: 0.7928 - val_accuracy: 0.6700: 0.6433 - accuracy: 0. - ETA: 45s - loss: 0.6433 - accuracy:  - ETA: 44s - los - ETA: 42s - loss: 0.6446 - ETA: 1s - loss: 0.6 - ETA: 1s - l\n",
      "Epoch 90/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6494 - accuracy: 0.6699 - val_loss: 1.0758 - val_accuracy: 0.6828\n",
      "Epoch 91/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6508 - accuracy: 0.6696 - val_loss: 0.8164 - val_accuracy: 0.6841: 0.6555 - accuracy: 0.66 - ETA: 16s - loss: 0. - ETA: 15\n",
      "Epoch 92/100\n",
      "1873/1873 [==============================] - 152s 81ms/step - loss: 0.6532 - accuracy: 0.6684 - val_loss: 0.4767 - val_accuracy: 0.6869 ETA: 28s - loss: 0.6591 - accura - ETA: 27s - los - ETA: 5s - loss: 0.6562 - accu - ETA:  - ETA: 0s - loss: 0.6\n",
      "Epoch 93/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6554 - accuracy: 0.6684 - val_loss: 0.2915 - val_accuracy: 0.6831ss: 0.6296 - a - ETA: 27s - loss: 0.6583 - accuracy: 0.66 - ETA: 27s - loss: 0.6582 - accuracy - ETA: 20s - loss: 0. - ETA: 18s - loss: 0.6598 - accurac - ETA: 17s - loss: 0.6594 - accura - ETA: 17s - loss: 0.6606 - accuracy: 0 - E\n",
      "Epoch 94/100\n",
      "1873/1873 [==============================] - 151s 81ms/step - loss: 0.6536 - accuracy: 0.6667 - val_loss: 0.3801 - val_accuracy: 0.6833los - E\n",
      "Epoch 95/100\n",
      "1873/1873 [==============================] - 149s 80ms/step - loss: 0.6563 - accuracy: 0.6664 - val_loss: 0.3466 - val_accuracy: 0.6870- loss: 0.640 - ET - ETA: 51s - loss: 0.6439 - accuracy: 0.66 - ETA: 50s - loss: - ETA: 49s - loss: 0. - ETA: 47s - loss: 0.6418 - accur - ETA: 46s - loss: 0 - ETA: 44s - l - ETA: 39s - loss: 0 - ETA: 37s - loss: 0.6456 -\n",
      "Epoch 96/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6511 - accuracy: 0.6694 - val_loss: 0.5409 - val_accuracy: 0.6857 ETA:  - ETA: 29s - loss: - ETA: 27s - loss: 0.6545 - accuracy: 0.6 - ETA: 27s - loss: 0.65\n",
      "Epoch 97/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6489 - accuracy: 0.6685 - val_loss: 0.5873 - val_accuracy: 0.6768\n",
      "Epoch 98/100\n",
      "1873/1873 [==============================] - 148s 79ms/step - loss: 0.6492 - accuracy: 0.6685 - val_loss: 0.5120 - val_accuracy: 0.6796\n",
      "Epoch 99/100\n",
      "1873/1873 [==============================] - 149s 79ms/step - loss: 0.6499 - accuracy: 0.6689 - val_loss: 0.5759 - val_accuracy: 0.6576\n",
      "Epoch 100/100\n",
      "1475/1873 [======================>.......] - ETA: 28s - loss: 0.6536 - accuracy: 0.6660- ETA: 56s - loss: 0.6379 - "
     ]
    }
   ],
   "source": [
    "for fold in range(nfolds): \n",
    "    print(\"Fold:\", str(fold))\n",
    "\n",
    "    file_h5 = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/\", str(fold),\n",
    "                                 \"/compounds_activity.h5\"))\n",
    "    f = h5py.File(file_h5, 'r')\n",
    "    group = '/activity'\n",
    "    table = \"prot_comp\"\n",
    "    ##### TEST\n",
    "    file_test = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/\", str(fold),\n",
    "                                 \"/compounds_activity_test.h5\"))\n",
    "    f_test = h5py.File(file_test, 'r')\n",
    "    \n",
    "    file_list = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/\", str(fold), \n",
    "                         \"/splitting_lists/splitting_\", str(fold),\"_list.pickle\"))\n",
    "    with open(file_list, \"rb\") as input_file:\n",
    "        splitting_list = pickle.load(input_file)\n",
    "    \n",
    "    splitting_list[0].sort()\n",
    "    splitting_list[1].sort()\n",
    "    test_indices = range(len(f_test[group][table]))\n",
    "    #splitting_list[2].sort()\n",
    "    \n",
    "    #Defining generators\n",
    "    train_generator = batch_generator_DL(batch_size, f, group, table, splitting_list[0], \n",
    "                                     max_len_prot, type_padding_prot=type_padding_prot)\n",
    "    val_generator = batch_generator_DL(batch_size, f, group, table, splitting_list[1], \n",
    "                                     max_len_prot, type_padding_prot=type_padding_prot)\n",
    "    \n",
    "    #defining callbacks\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/logs/\", str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/logs/\", str(fold), \"/\")))\n",
    "    \n",
    "    log_path = \"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/logs/\", str(fold), \"/training_log.csv\"))\n",
    "    csv_logger = CSVLogger(log_path)\n",
    "\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/checkpoint/\", str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/checkpoint/\", str(fold), \"/\")))\n",
    "\n",
    "    #if there are already files in the folder, it removes them\n",
    "    r = glob(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/checkpoint/\", str(fold), \"/*\")))\n",
    "    for i in r:\n",
    "        os.remove(i)\n",
    "   \n",
    "    terminan = TerminateOnNaN()\n",
    "    checkpoint_path = \"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/semi_resampling/checkpoint/\", str(fold),\n",
    "                               \"/weights-improvement-{epoch:03d}-{val_accuracy:.4f}.hdf5\"))\n",
    "    mcheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=0, \n",
    "                                          save_best_only=True, save_weights_only=False)\n",
    "\n",
    "    callbacks_list = [csv_logger, terminan, mcheckpoint ]\n",
    "    print(\"Training\")\n",
    "    # fitting the model\n",
    "    history = model.fit_generator(generator=train_generator, \n",
    "                              validation_data=val_generator,\n",
    "                             steps_per_epoch= int(len(splitting_list[0])/batch_size),\n",
    "                              validation_steps=int(len(splitting_list[1])/batch_size),\n",
    "                             epochs=epochss,\n",
    "                             callbacks=callbacks_list,\n",
    "                             verbose=1)\n",
    "    #saving history\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", \n",
    "                                   str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", str(fold), \"/\")))\n",
    "\n",
    "    with open(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", str(fold), \"/history.pickle\")), 'wb') as handle:\n",
    "        pickle.dump(history, handle)\n",
    "        \n",
    "    print(\"Prediction on test data\")\n",
    "    #PROTEINS\n",
    "    batch_sequences = list(f_test[group][table][test_indices][\"sequence\"])\n",
    "    #COMPOUNDS\n",
    "    batch_compounds = list(f_test[group][table][test_indices][\"fingerprint\"])\n",
    "    #LABELS\n",
    "    batch_y = list(f_test[group][table][test_indices][\"label\"])\n",
    "    #processing sequences and compounds\n",
    "    seqs_onehot = np.asarray(processing_sequences(batch_sequences, max_len_prot, type_padding_prot))\n",
    "    comps_batch = np.asarray(processing_fingerprints(batch_compounds))\n",
    "    batch_labels = np.asarray(bin_to_onehot(batch_y))\n",
    "    \n",
    "    history_path = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", \n",
    "                                   str(fold), \"/history.pickle\"))\n",
    "    path_to_confusion = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", \n",
    "                                   str(fold), \"/\"))\n",
    "    path_to_auc = \"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", \n",
    "                                   str(fold), \"/\"))\n",
    "    \n",
    "    history = plot_history(history_path, \"\".join((absPath, \"data/\", protein_type, \n",
    "                                                  \"/semi_resampling/results/\", \n",
    "                                   str(fold), \"/\")))\n",
    "    path_to_cp = ''.join((absPath, \"data/\", protein_type, \"/semi_resampling/checkpoint/\", \n",
    "                          str(fold), \"/\"))\n",
    "\n",
    "    model, best_path = load_best_model(history, path_to_cp)\n",
    "\n",
    "    cps_loc = ''.join((absPath, \"data/\", protein_type, \"/semi_resampling/checkpoint/\", \n",
    "                          str(fold), \"/*.hdf5\")) \n",
    "\n",
    "    #removing the rest of weights\n",
    "    fileList = glob(cps_loc, recursive=True)\n",
    "    fileList.remove(best_path)\n",
    "    if len(fileList) >1:\n",
    "        for filePath in fileList:\n",
    "            try:\n",
    "                os.remove(filePath)\n",
    "            except OSError:\n",
    "                print(\"Error while deleting file\")\n",
    "    \n",
    "    y_predprob = model.predict([seqs_onehot, comps_batch])\n",
    "    y_prob = y_predprob[:,1]\n",
    "    y_pred = y_predprob.argmax(-1)\n",
    "    y_test = batch_labels.argmax(-1)\n",
    "    print(\"Counting predicted: \", Counter(y_pred))\n",
    "    \n",
    "    batch_compID_test = list(f_test[group][table][test_indices][\"da_comp_id\"])\n",
    "    batch_protID_test = list(f_test[group][table][test_indices][\"da_prot_id\"])\n",
    "    \n",
    "    #confusion matrix\n",
    "    confusion_matrix(y_test, y_pred, path_to_confusion)\n",
    "        \n",
    "    #AUC\n",
    "    file_auc = ''.join((absPath, \"data/\", protein_type, \"/semi_resampling/results/\", \n",
    "                                   str(fold), \"/AUC.pickle\"))\n",
    "    compute_roc(y_test, y_prob, path_to_auc)\n",
    "    \n",
    "    # saving predictions on test set\n",
    "\n",
    "    predictions_test = pd.DataFrame({\"y_test\":y_test, \"y_prob\":y_prob, \"y_pred\":y_pred, \"comp_ID\": batch_compID_test,\n",
    "                                \"DeepAffinity Protein ID\": batch_protID_test})\n",
    "\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/predictions/\", str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/predictions/\", str(fold), \"/\")))\n",
    "\n",
    "    predictions_test.to_csv(\"\".join((absPath, \"data/\", protein_type, \"/semi_resampling/predictions/\", str(fold), \"/test.csv\")))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
