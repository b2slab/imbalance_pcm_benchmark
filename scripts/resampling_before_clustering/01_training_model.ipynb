{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "from sklearn import metrics \n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tables import *\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.backend import manual_variable_initialization \n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, concatenate, Flatten, Conv1D, BatchNormalization, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, TerminateOnNaN\n",
    "#import keras\n",
    "\n",
    "#root\n",
    "absPath = '/home/angela3/imbalance_pcm_benchmark/'\n",
    "sys.path.insert(0, absPath)\n",
    "\n",
    "\n",
    "from src.model_functions import *\n",
    "from src.Target import Target\n",
    "from src.postproc_auxiliar_functions import *\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0' \n",
    "np.random.seed(8)\n",
    "random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/angela3/imbalance_pcm_benchmark/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "batch_size = 128\n",
    "epochss = 100\n",
    "type_padding_prot = \"pre_padding\"\n",
    "protein_type= \"GPCRs\" #\"kinases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening HDF5 with data\n",
    "file_h5 = \"\".join((absPath, \"data/\", protein_type,\"/resampling_before_clustering/compounds_activity.h5\"))\n",
    "f = h5py.File(file_h5, 'r')\n",
    "group = '/activity'\n",
    "table = \"prot_comp\"\n",
    "\n",
    "#Loading maximum lengths of proteins and compounds\n",
    "with open(\"\".join((absPath, 'data/prot_max_len.pickle')), \"rb\") as input_file:\n",
    "    max_len_prot = pickle.load(input_file)\n",
    "#Defining protein dictionary    \n",
    "instarget = Target(\"AAA\")\n",
    "prot_dict = instarget.predefining_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "decay_rate = learning_rate/epochss\n",
    "adamm = Adam(lr=learning_rate, beta_1=0.1, beta_2=0.001, epsilon=1e-08, decay=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1499, 26)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1499, 64)     5056        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1499, 64)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 95936)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 881)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           4796850     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           44100       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100)          0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            202         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,846,208\n",
      "Trainable params: 4,846,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LEFT BLOCK (to analyse amino acid sequences)\n",
    "input_seq = Input(shape=(max_len_prot, len(prot_dict)), dtype='float32')\n",
    "conv_seq = Conv1D(filters=64, padding='same', strides=1, kernel_size=3, activation='relu')(input_seq)\n",
    "dropout_1 = Dropout(0.4)(conv_seq)\n",
    "flatten_seq = Flatten()(dropout_1)#(dense_seq)\n",
    "dense_seq_2 = Dense(50)(flatten_seq)\n",
    "dropout_2 = Dropout(0.4)(dense_seq_2)\n",
    "\n",
    "#RIGHT BRANCH (to analyse fingerprints)\n",
    "input_fps = Input(shape=(881,), dtype='float32')\n",
    "dense_fps = Dense(50)(input_fps)\n",
    "dropout_3 = Dropout(0.4)(dense_fps)\n",
    "#bn_3 =  BatchNormalization()(dense_fps)#(dense_seq_2)#(conv_seq)\n",
    "\n",
    "\n",
    "#MERGE BOTH BRANCHES\n",
    "main_merged = concatenate([dropout_2, dropout_3],axis=1)#([dense_seq_2, dense_fps], axis=1)\n",
    "\n",
    "main_dense = Dense(2, activation='softmax')(main_merged)\n",
    "\n",
    "#build and compile model\n",
    "model = Model(inputs=[input_seq, input_fps], outputs=[main_dense])\n",
    "model.compile(loss='categorical_crossentropy', optimizer = adamm, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Training\n",
      "Epoch 1/100\n",
      "1906/1906 [==============================] - 155s 82ms/step - loss: 1.9364 - accuracy: 0.6134 - val_loss: 0.6333 - val_accuracy: 0.5588y\n",
      "Epoch 2/100\n",
      "  59/1906 [..............................] - ETA: 2:03 - loss: 0.6453 - accuracy: 0.6396"
     ]
    }
   ],
   "source": [
    "for fold in range(nfolds): \n",
    "    print(\"Fold:\", str(fold))\n",
    "    file_list = \"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/splitting_lists/splitting_\",\n",
    "                               str(fold), \"_list.pickle\"))\n",
    "    with open(file_list, \"rb\") as input_file:\n",
    "        splitting_list = pickle.load(input_file)    \n",
    "    \n",
    "    splitting_list[0].sort()\n",
    "    splitting_list[1].sort()\n",
    "    #Defining generators\n",
    "    train_generator = batch_generator_DL(batch_size, f, group, table, splitting_list[0], \n",
    "                                     max_len_prot, type_padding_prot=type_padding_prot)\n",
    "    val_generator = batch_generator_DL(batch_size, f, group, table, splitting_list[1], \n",
    "                                     max_len_prot, type_padding_prot=type_padding_prot)\n",
    "    \n",
    "    #defining callbacks\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/logs/\", str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/logs/\", str(fold), \"/\")))\n",
    "    \n",
    "    log_path = \"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/logs/\", str(fold), \"/training_log.csv\"))\n",
    "    csv_logger = CSVLogger(log_path)\n",
    "\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/checkpoint/\", str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/checkpoint/\", str(fold), \"/\")))\n",
    "\n",
    "    #if there are already files in the folder, it removes them\n",
    "    r = glob(\"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/checkpoint/\", str(fold), \"/*\")))\n",
    "    for i in r:\n",
    "        os.remove(i)\n",
    "   \n",
    "    terminan = TerminateOnNaN()\n",
    "    checkpoint_path = \"\".join((absPath, \"data/\", protein_type, \n",
    "                                   \"/resampling_before_clustering/checkpoint/\", str(fold),\n",
    "                               \"/weights-improvement-{epoch:03d}-{val_accuracy:.4f}.hdf5\"))\n",
    "    mcheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=0, \n",
    "                                          save_best_only=True, save_weights_only=False)\n",
    "\n",
    "    callbacks_list = [csv_logger, terminan, mcheckpoint ]\n",
    "    print(\"Training\")\n",
    "    # fitting the model\n",
    "    history = model.fit_generator(generator=train_generator, \n",
    "                              validation_data=val_generator,\n",
    "                             steps_per_epoch= int(len(splitting_list[0])/batch_size),\n",
    "                              validation_steps=int(len(splitting_list[1])/batch_size),\n",
    "                             epochs=epochss,\n",
    "                             callbacks=callbacks_list,\n",
    "                             verbose=1)\n",
    "    #saving history\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", \n",
    "                                   str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", str(fold), \"/\")))\n",
    "\n",
    "    with open(\"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", str(fold), \"/history.pickle\")), 'wb') as handle:\n",
    "        pickle.dump(history, handle)\n",
    "        \n",
    "    print(\"Prediction on test data\")\n",
    "    splitting_list[2].sort()\n",
    "    #PROTEINS\n",
    "    batch_sequences = list(f[group][table][splitting_list[2]][\"sequence\"])\n",
    "    #COMPOUNDS\n",
    "    batch_compounds = list(f[group][table][splitting_list[2]][\"fingerprint\"])\n",
    "    #LABELS\n",
    "    batch_y = list(f[group][table][splitting_list[2]][\"label\"])\n",
    "    #processing sequences and compounds\n",
    "    seqs_onehot = np.asarray(processing_sequences(batch_sequences, max_len_prot, type_padding_prot))\n",
    "    comps_batch = np.asarray(processing_fingerprints(batch_compounds))\n",
    "    batch_labels = np.asarray(bin_to_onehot(batch_y))\n",
    "    \n",
    "    history_path = \"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", \n",
    "                                   str(fold), \"/history.pickle\"))\n",
    "    path_to_confusion = \"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", \n",
    "                                   str(fold), \"/\"))\n",
    "    path_to_auc = \"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", \n",
    "                                   str(fold), \"/\"))\n",
    "    \n",
    "    history = plot_history(history_path, \"\".join((absPath, \"data/\", protein_type, \n",
    "                                                  \"/resampling_before_clustering/results/\", \n",
    "                                   str(fold), \"/\")))\n",
    "    path_to_cp = ''.join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/checkpoint/\", \n",
    "                          str(fold), \"/\"))\n",
    "\n",
    "    model, best_path = load_best_model(history, path_to_cp)\n",
    "\n",
    "    cps_loc = ''.join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/checkpoint/\", \n",
    "                          str(fold), \"/*.hdf5\")) \n",
    "\n",
    "    #removing the rest of weights\n",
    "    fileList = glob(cps_loc, recursive=True)\n",
    "    fileList.remove(best_path)\n",
    "    if len(fileList) >1:\n",
    "        for filePath in fileList:\n",
    "            try:\n",
    "                os.remove(filePath)\n",
    "            except OSError:\n",
    "                print(\"Error while deleting file\")\n",
    "    \n",
    "    y_predprob = model.predict([seqs_onehot, comps_batch])\n",
    "    y_prob = y_predprob[:,1]\n",
    "    y_pred = y_predprob.argmax(-1)\n",
    "    y_test = batch_labels.argmax(-1)\n",
    "    print(\"Counting predicted: \", Counter(y_pred))\n",
    "    \n",
    "    batch_compID_test = list(f[group][table][splitting_list[2]][\"da_comp_id\"])\n",
    "    batch_protID_test = list(f[group][table][splitting_list[2]][\"da_prot_id\"])\n",
    "    \n",
    "    #confusion matrix\n",
    "    confusion_matrix(y_test, y_pred, path_to_confusion)\n",
    "        \n",
    "    #AUC\n",
    "    file_auc = ''.join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/results/\", \n",
    "                                   str(fold), \"/AUC.pickle\"))\n",
    "    compute_roc(y_test, y_prob, path_to_auc)\n",
    "    \n",
    "    # saving predictions on test set\n",
    "\n",
    "    predictions_test = pd.DataFrame({\"y_test\":y_test, \"y_prob\":y_prob, \"y_pred\":y_pred, \"comp_ID\": batch_compID_test,\n",
    "                                \"DeepAffinity Protein ID\": batch_protID_test})\n",
    "\n",
    "    if not os.path.exists(\"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/predictions/\", str(fold), \"/\"))):\n",
    "        os.makedirs(\"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/predictions/\", str(fold), \"/\")))\n",
    "\n",
    "    predictions_test.to_csv(\"\".join((absPath, \"data/\", protein_type, \"/resampling_before_clustering/predictions/\", str(fold), \"/test.csv\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
